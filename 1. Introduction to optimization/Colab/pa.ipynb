{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment (Linear models, Optimization)\n",
    "\n",
    "In this programming assignment you will implement a linear classifier and train it using stochastic gradient descent modifications and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-dimensional classification\n",
    "\n",
    "To make things more intuitive, let's solve a 2D classification problem with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "# keep random_state=42 for deterministic results\n",
    "(X, y) = datasets.make_circles(n_samples=1024, shuffle=True, noise=0.2, factor=0.4, random_state=42)\n",
    "ind = np.logical_or(y == 1, X[:, 1] > X[:, 0] - 0.5)\n",
    "X = X[ind, :]\n",
    "m = np.array([[1, 1], [-2, 1]])\n",
    "X = preprocessing.scale(X)\n",
    "y = y[ind]\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "## Features\n",
    "\n",
    "As you can notice the data above isn't linearly separable. Since that we should add features (or use non-linear model). Note that decision line between two classes have form of circle, since that we can add quadratic features to make the problem linearly separable. The idea under this displayed on image below:\n",
    "\n",
    "![](kernel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand(X):\n",
    "    \"\"\"\n",
    "    Adds quadratic features. \n",
    "    This expansion allows your linear model to make non-linear separation.\n",
    "    \n",
    "    For each sample (row in matrix), compute an expanded row:\n",
    "    [feature0, feature1, feature0^2, feature1^2, feature1*feature2, 1]\n",
    "    \n",
    "    :param X: matrix of features, shape [n_samples,2]\n",
    "    :returns: expanded features of shape [n_samples,6]\n",
    "    \"\"\"\n",
    "    X_expanded = np.zeros((X.shape[0], 6))\n",
    "    \n",
    "    #TODO: <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_expanded = expand(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some tests for your implementation of `expand` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple test on random numbers\n",
    "\n",
    "dummy_X = np.array([\n",
    "        [0,0],\n",
    "        [1,0],\n",
    "        [2.61,-1.28],\n",
    "        [-0.59,2.1]\n",
    "    ])\n",
    "\n",
    "#call your expand function\n",
    "dummy_expanded = expand(dummy_X)\n",
    "\n",
    "#what it should have returned:   x0       x1       x0^2     x1^2     x0*x1    1\n",
    "dummy_expanded_ans = np.array([[ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  1.    ],\n",
    "                               [ 1.    ,  0.    ,  1.    ,  0.    ,  0.    ,  1.    ],\n",
    "                               [ 2.61  , -1.28  ,  6.8121,  1.6384, -3.3408,  1.    ],\n",
    "                               [-0.59  ,  2.1   ,  0.3481,  4.41  , -1.239 ,  1.    ]])\n",
    "\n",
    "#tests\n",
    "assert isinstance(dummy_expanded,np.ndarray), \"please make sure you return numpy array\"\n",
    "assert dummy_expanded.shape==dummy_expanded_ans.shape, \"please make sure your shape is correct\"\n",
    "assert np.allclose(dummy_expanded,dummy_expanded_ans,1e-3), \"Something's out of order with features\"\n",
    "\n",
    "print(\"Seems legit!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "To classify objects we will obtain probability of object belongs to class '1'. To predict probability we will use output of linear model and logistic function:\n",
    "\n",
    "$$ a(x; w) = \\langle w, x \\rangle $$\n",
    "$$ P( y=1 \\; \\big| \\; x, \\, w) = \\dfrac{1}{1 + \\exp(- \\langle w, x \\rangle)} = \\sigma(\\langle w, x \\rangle)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability(X, w):\n",
    "    \"\"\"\n",
    "    Given input features and weights\n",
    "    return predicted probabilities of y==1 given x, P(y=1|x), see description above\n",
    "        \n",
    "    Don't forget to use expand(X) function (where necessary) in this and subsequent functions.\n",
    "    \n",
    "    :param X: feature matrix X of shape [n_samples,2] (non-expanded)\n",
    "    :param w: weight vector w of shape [6] for each of the expanded features\n",
    "    :returns: an array of predicted probabilities in [0,1] interval.\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use output of this cell to fill answer field \n",
    "\n",
    "dummy_weights = np.linspace(-1, 1, 6)\n",
    "probability(X_expanded[:1,:], dummy_weights)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression the optimal parameters $w$ are found by cross-entropy minimization:\n",
    "\n",
    "$$ L(w) =  - {1 \\over N} \\sum_{i=1}^N \\left[ {y_i \\cdot log P(y_i \\, | \\, x_i,w) + (1-y_i) \\cdot log (1-P(y_i\\, | \\, x_i,w))}\\right] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,2], target vector [n_samples] of 1/0,\n",
    "    and weight vector w [6], compute scalar loss function using formula above.\n",
    "    \"\"\"\n",
    "    #TODO: <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use output of this cell to fill answer field \n",
    "compute_loss(X_expanded, y, dummy_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we train our model with gradient descent, we should compute gradients.\n",
    "\n",
    "To be specific, we need a derivative of loss function over each weight [6 of them].\n",
    "\n",
    "$$ \\nabla_w L = ...$$\n",
    "\n",
    "We won't be giving you the exact formula this time â€” instead, try figuring out a derivative with pen and paper. \n",
    "\n",
    "As usual, we've made a small test for you, but if you need more, feel free to check your math against finite differences (estimate how $L$ changes if you shift $w$ by $10^{-5}$ or so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,6], target vector [n_samples] of +1/-1,\n",
    "    and weight vector w [6], compute vector [6] of derivatives of L over each weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use output of this cell to fill answer field \n",
    "np.linalg.norm(compute_grad(X_expanded, y, dummy_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an auxiliary function that visualizes the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "def visualize(X, y, w, history):\n",
    "    \"\"\"draws classifier prediction with matplotlib magic\"\"\"\n",
    "    Z = probability(expand(np.c_[xx.ravel(), yy.ravel()]), w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history)\n",
    "    plt.grid()\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.ylim(0, ymax)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(X, y, dummy_weights, [0.5,0.5,0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "In this section we'll use the functions you wrote to train our classifier using stochastic gradient descent.\n",
    "\n",
    "You can try change hyperparameters like batch size, learning rate and so on to find the best one, but use our hyperparameters when fill answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "Stochastic gradient descent just takes a random example on each iteration, calculates a gradient of the loss on it and makes a step:\n",
    "$$ w_t = w_{t-1} - \\eta \\dfrac{1}{m} \\sum_{j=1}^m \\nabla_w L(w_t, x_{i_j}, y_{i_j}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0,0,0,0,0,1])\n",
    "\n",
    "eta= 0.1 # learning rate\n",
    "\n",
    "n_iter = 100\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X_expanded.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X_expanded, y, w)\n",
    "    visualize(X_expanded[ind,:], y[ind], w, loss)\n",
    "    #TODO: <your code here>\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use output of this cell to fill answer field \n",
    "\n",
    "compute_loss(X_expanded, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with momentum\n",
    "\n",
    "Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in image below. It does this by adding a fraction $\\alpha$ of the update vector of the past time step to the current update vector.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ \\nu_t = \\alpha \\nu_{t-1} + \\eta\\dfrac{1}{m} \\sum_{j=1}^m \\nabla_w L(w_t, x_{i_j}, y_{i_j}) $$\n",
    "$$ w_t = w_{t-1} - \\nu_t$$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "![](sgd.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0,0,0,0,0,1])\n",
    "\n",
    "eta = 0.05 # learning rate\n",
    "alpha = 0.9 # momentum\n",
    "nu = np.zeros_like(w)\n",
    "\n",
    "n_iter = 100\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X_expanded.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X_expanded, y, w)\n",
    "    visualize(X_expanded[ind,:], y[ind], w, loss)    \n",
    "    #TODO: <your code here>\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use output of this cell to fill answer field \n",
    "\n",
    "compute_loss(X_expanded, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "\n",
    "Implement RMSPROP algorithm, which use squared gradients to adjust learning rate:\n",
    "\n",
    "$$ G_j^t = \\alpha G_j^{t-1} + (1 - \\alpha) g_{tj}^2 $$\n",
    "$$ w_j^t = w_j^{t-1} - \\dfrac{\\eta}{\\sqrt{G_j^t + \\varepsilon}} g_{tj} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0, 0, 0, 0, 0, 1.])\n",
    "\n",
    "eta = 0.1 # learning rate\n",
    "alpha = 0.9 #moving average of gradient norm squared\n",
    "g2 = None\n",
    "eps = 1e-8\n",
    "\n",
    "n_iter = 100\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12,5))\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X_expanded.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X_expanded, y, w)\n",
    "    visualize(X_expanded[ind,:], y[ind], w, loss)\n",
    "    #TODO: <your code here>\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use output of this cell to fill answer field \n",
    "\n",
    "compute_loss(X_expanded, y, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
